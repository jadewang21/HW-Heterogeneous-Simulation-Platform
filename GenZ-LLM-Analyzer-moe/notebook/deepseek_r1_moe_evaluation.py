#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DeepSeek-R1 MoE æ¨¡å‹æ€§èƒ½è¯„ä¼°è„šæœ¬

å‚è€ƒ a6000.py çš„è¯„ä¼°æ–¹å¼ï¼Œä¸“é—¨é’ˆå¯¹ MoE æ¨¡å‹çš„æ€§èƒ½è¯„ä¼°
æ”¯æŒç”¨æˆ·çµæ´»æŒ‡å®šç¡¬ä»¶é…ç½®ï¼Œè®¡ç®— TTFTã€TPOTã€E2E å’Œååé‡ï¼Œä¿å­˜åˆ° CSV
"""

import sys
import os
import json
import numpy as np
import pandas as pd
import warnings
import argparse
from tqdm import tqdm

# å¯¼å…¥GenZæ ¸å¿ƒæ¨¡å—  
import sys
sys.path.append('/home/wang/sim/org-genz/GenZ-LLM-Analyzer')

from GenZ import (
    decode_moddeling, 
    prefill_moddeling,
    ModelConfig,
    System
)
from GenZ.Models.default_models import MODEL_DICT
from Systems.system_configs import system_configs

class DeepSeekR1MoEEvaluator:
    """DeepSeek-R1 MoEæ¨¡å‹æ€§èƒ½è¯„ä¼°å™¨"""
    
    def __init__(self, config_path="ds-r1-config.json"):
        """åˆå§‹åŒ–è¯„ä¼°å™¨ï¼ŒåŠ è½½çœŸå®é…ç½®"""
        
        # è¯»å–DeepSeek-R1é…ç½®
        if os.path.exists(config_path):
            with open(config_path, 'r') as f:
                self.config_json = json.load(f)
        else:
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {config_path}")
        
        # åˆ›å»ºå¹¶æ³¨å†Œæ¨¡å‹
        self._create_and_register_model()
        
        print("âœ“ DeepSeek-R1 MoEæ¨¡å‹é…ç½®å·²åŠ è½½")
        print(f"  - å±‚æ•°: {self.config_json['num_hidden_layers']}")
        print(f"  - éšè—ç»´åº¦: {self.config_json['hidden_size']} ")
        print(f"  - MoEä¸“å®¶æ•°: {self.config_json['n_routed_experts']}")
        print(f"  - æ¿€æ´»ä¸“å®¶æ•°: {self.config_json['num_experts_per_tok']}")
        print(f"  - è¯æ±‡è¡¨: {self.config_json['vocab_size']}")
    
    def _create_and_register_model(self):
        """åˆ›å»ºå¹¶æ³¨å†Œæ¨¡å‹é…ç½®"""
        config = self.config_json
        
        model_config = ModelConfig(
            model='deepseek-r1',
            vocab_size=config['vocab_size'],
            max_model_len=config['max_position_embeddings'], 
            hidden_size=config['hidden_size'],
            intermediate_size=config['intermediate_size'],
            num_decoder_layers=config['num_hidden_layers'],
            num_attention_heads=config['num_attention_heads'],
            num_key_value_heads=config['num_key_value_heads'],
            head_dim=config['v_head_dim'],
            hidden_act=config['hidden_act'],
            
            # MoE æ ¸å¿ƒé…ç½®
            num_experts=config['n_routed_experts'],
            expert_top_k=config['num_experts_per_tok'],
            moe_intermediate_size=config['moe_intermediate_size'],
            n_shared_experts=config['n_shared_experts'],
            shared_expert_intermediate_size=config['intermediate_size'],
            first_k_dense_replace=config['first_k_dense_replace'],
            moe_layer_freq=config['moe_layer_freq']
        )
        
        try:
            MODEL_DICT.add_model(model_config)
            print("âœ“ æ¨¡å‹å·²æ³¨å†Œåˆ°GenZç³»ç»Ÿ")
        except Exception as e:
            print(f"æ¨¡å‹æ³¨å†Œè­¦å‘Š: {e}")
    
    def run_batch_simulation_from_df(self,
                                    df,
                                    hardware_config,
                                    quantization_setting='fp8',
                                    beam_size_value=1,
                                    system_efficiency_value=0.8,
                                    tensor_parallel_nodes=8,
                                    pipeline_parallel_nodes=1,
                                    model_offload_enabled=False,
                                    output_csv_filename='deepseek_r1_results.csv',
                                    # å¤šèŠ‚ç‚¹é…ç½®å‚æ•°
                                    num_nodes=1,
                                    interchip_link_bw=450,
                                    interchip_link_latency=1.9,
                                    topology='FullyConnected'):
        """
        æ‰¹é‡æ¨¡æ‹Ÿè¯„ä¼°ï¼Œå‚è€ƒ a6000.py çš„å®ç°æ–¹å¼
        ä¸“é—¨é’ˆå¯¹ MoE æ¨¡å‹è¿›è¡Œä¼˜åŒ–
        """
        
        warnings.filterwarnings("ignore")
        
        # éªŒè¯å¿…è¦åˆ—
        required_columns = ['input_tokens', 'output_tokens']
        for col in required_columns:
            if col not in df.columns:
                raise ValueError(f"CSVç¼ºå°‘å¿…è¦çš„åˆ—: {col}")
        
        print(f"æ‰¾åˆ° {len(df)} ä¸ªè¯·æ±‚ï¼Œå¼€å§‹ DeepSeek-R1 MoE æ¨¡æ‹Ÿ")
        print(f"å¹¶è¡Œé…ç½®: TP={tensor_parallel_nodes}, PP={pipeline_parallel_nodes}")
        print(f"å¤šèŠ‚ç‚¹é…ç½®: {num_nodes}ä¸ªèŠ‚ç‚¹, æ‹“æ‰‘={topology}")
        print(f"èŠ‚ç‚¹äº’è¿: {interchip_link_bw}GB/så¸¦å®½, {interchip_link_latency}Î¼så»¶è¿Ÿ")
        print(f"é‡åŒ–è®¾ç½®: {quantization_setting}")
        print(f"å†…å­˜å¸è½½: {'å¯ç”¨' if model_offload_enabled else 'ç¦ç”¨'}")
        
        # å…³é”®ä¿®å¤ï¼šæ­£ç¡®æ„å»ºGenZå¤šèŠ‚ç‚¹ç³»ç»Ÿå¯¹è±¡
        # 4ä¸ªèŠ‚ç‚¹ï¼Œæ¯ä¸ªèŠ‚ç‚¹8å¼ å¡ï¼Œæ€»å…±32å¼ å¡
        cards_per_node = 8
        total_cards = num_nodes * cards_per_node
        
        # æ„å»ºGenZ Systemå¯¹è±¡ï¼ŒåŒ…å«å®Œæ•´çš„å¤šèŠ‚ç‚¹é…ç½®
        print(f"ğŸ”§ æ„å»ºå¤šèŠ‚ç‚¹GenZç³»ç»Ÿ...")
        print(f"   é…ç½®ï¼š{num_nodes}èŠ‚ç‚¹ Ã— {cards_per_node}å¡/èŠ‚ç‚¹ = {total_cards}å¼ å¡")
        print(f"   èŠ‚ç‚¹äº’è¿ï¼š{interchip_link_bw} GB/s, å»¶è¿Ÿï¼š{interchip_link_latency} Î¼s")
        print(f"   æ‹“æ‰‘ç»“æ„ï¼š{topology}")
        
        # å…³é”®ä¿®å¤ï¼šæ­£ç¡®æ„å»ºGenZå¤šèŠ‚ç‚¹ç³»ç»Ÿé…ç½®
        # é¦–å…ˆæ·»åŠ RTX 4090é…ç½®åˆ°ç³»ç»Ÿé…ç½®å­—å…¸
        if 'RTX_4090_GPU' not in system_configs:
            system_configs['RTX_4090_GPU'] = {
                'Flops': hardware_config['Flops'], 
                'Memory_size': hardware_config['Memory_size'], 
                'Memory_BW': hardware_config['Memory_BW'], 
                'ICN': hardware_config.get('ICN', 16), 
                'real_values': True
            }
        
        # æ­£ç¡®æ„å»ºå¤šèŠ‚ç‚¹Systemå¯¹è±¡
        system = System(
            flops=hardware_config['Flops'],  # å•å¡ç®—åŠ›ï¼ŒGenZä¼šè‡ªåŠ¨å¤„ç†å¤šèŠ‚ç‚¹èšåˆ
            off_chip_mem_size=hardware_config['Memory_size'] * 1024,  # å•å¡å†…å­˜(MB)
            offchip_mem_bw=hardware_config['Memory_BW'],  # å•å¡å¸¦å®½
            interchip_link_bw=interchip_link_bw,  # èŠ‚ç‚¹é—´äº’è¿å¸¦å®½
            interchip_link_latency=interchip_link_latency,  # èŠ‚ç‚¹é—´å»¶è¿Ÿ
            bits=quantization_setting,
            compute_efficiency=system_efficiency_value,
            memory_efficiency=system_efficiency_value,
            comm_efficiency=system_efficiency_value,
            num_nodes=num_nodes,  # GenZä¼šæ ¹æ®æ­¤å‚æ•°è‡ªåŠ¨è®¡ç®—æ€»èµ„æº
            topology=topology,
            collective_strategy='GenZ',
            parallelism_heirarchy=f"TP{{{tensor_parallel_nodes}}}_EP{{1}}_PP{{{pipeline_parallel_nodes}}}"
        )
        
        print(f"âœ… ç³»ç»Ÿæ„å»ºå®Œæˆï¼š")
        print(f"   å•å¡ç®—åŠ›ï¼š{system.unit.raw_to_unit(system.flops, type='C')} TFLOPs")
        print(f"   å•å¡å†…å­˜ï¼š{system.unit.raw_to_unit(system.off_chip_mem_size, type='M')/1024:.1f} GB")
        print(f"   é›†ç¾¤æ€»ç®—åŠ›ï¼š{system.unit.raw_to_unit(system.flops, type='C') * total_cards} TFLOPs")
        print(f"   é›†ç¾¤æ€»å†…å­˜ï¼š{system.unit.raw_to_unit(system.off_chip_mem_size, type='M')/1024 * total_cards:.1f} GB")
        print(f"   èŠ‚ç‚¹é—´å¸¦å®½ï¼š{system.unit.raw_to_unit(system.interchip_link_bw, type='BW')} GB/s")
        
        results = []
        
        # æ„å»ºå¹¶è¡Œå±‚çº§ç»“æ„å­—ç¬¦ä¸² (å‚è€ƒ a6000.py)
        parallelism_heirarchy = f"TP{{{tensor_parallel_nodes}}}_EP{{1}}_PP{{{pipeline_parallel_nodes}}}"
        
        # é€ä¸ªè¯·æ±‚è¿›è¡Œè¯„ä¼°
        for idx, row in tqdm(df.iterrows(), total=len(df), desc="MoEæ¨¡æ‹Ÿè¿›åº¦"):
            input_tokens = int(row['input_tokens'])
            output_tokens = int(row['output_tokens'])
            request_id = idx
            batch_size = 1  # MoEæ¨¡å‹é€šå¸¸ä½¿ç”¨è¾ƒå°çš„batch size
            
            # åˆå§‹åŒ–ç»“æœ
            ttft_ms = np.nan
            tpot_ms = np.nan
            e2e_ms = np.nan
            prefill_throughput = np.nan
            decode_throughput = np.nan
            notes = ""
            
            try:
                # 1. é¢„å¡«å……é˜¶æ®µè¯„ä¼°
                try:
                    prefill_outputs = prefill_moddeling(
                        model='deepseek-r1',
                        batch_size=batch_size,
                        input_tokens=input_tokens,
                        system_name=system,  # ä½¿ç”¨æ„å»ºçš„Systemå¯¹è±¡
                        system_eff=system_efficiency_value,
                        bits=quantization_setting,
                        tensor_parallel=tensor_parallel_nodes,
                        pipeline_parallel=pipeline_parallel_nodes,
                        parallelism_heirarchy=parallelism_heirarchy,
                        model_offload=model_offload_enabled,
                        debug=False
                    )
                    
                    ttft_ms = prefill_outputs.get('Latency', np.nan)
                    prefill_throughput = prefill_outputs.get('Throughput', np.nan)
                    
                except Exception as prefill_error:
                    print(f"è¯·æ±‚ {request_id}: é¢„å¡«å……å¤±è´¥ - {prefill_error}")
                    ttft_ms = np.nan
                    prefill_throughput = np.nan
                
                # 2. è§£ç é˜¶æ®µè¯„ä¼°
                try:
                    decode_outputs = decode_moddeling(
                        model='deepseek-r1',
                        batch_size=batch_size,
                        Bb=beam_size_value,
                        input_tokens=input_tokens,
                        output_tokens=output_tokens,
                        system_name=system,  # ä½¿ç”¨æ„å»ºçš„Systemå¯¹è±¡
                        system_eff=system_efficiency_value,
                        bits=quantization_setting,
                        tensor_parallel=tensor_parallel_nodes,
                        pipeline_parallel=pipeline_parallel_nodes,
                        parallelism_heirarchy=parallelism_heirarchy,
                        model_offload=model_offload_enabled,
                        debug=False
                    )
                    
                    decode_latency = decode_outputs.get('Latency', np.nan)
                    decode_throughput = decode_outputs.get('Throughput', np.nan)
                    
                    if not np.isnan(decode_latency) and output_tokens > 0:
                        tpot_ms = decode_latency / output_tokens
                    else:
                        tpot_ms = np.nan
                        
                except Exception as decode_error:
                    print(f"è¯·æ±‚ {request_id}: è§£ç å¤±è´¥ - {decode_error}")
                    tpot_ms = np.nan
                    decode_throughput = np.nan
                
                # 3. è®¡ç®—ç«¯åˆ°ç«¯æŒ‡æ ‡
                if not np.isnan(ttft_ms) and not np.isnan(tpot_ms):
                    e2e_ms = ttft_ms + (tpot_ms * output_tokens)
                else:
                    e2e_ms = np.nan
                    
                # å¤„ç†å†…å­˜ä¸è¶³çš„æƒ…å†µ (å‚è€ƒ a6000.py çš„é”™è¯¯å¤„ç†)
            except ValueError as ve:
                if "All params would not fit on chip" in str(ve) and not model_offload_enabled:
                    print(f"è¯·æ±‚ {request_id}: å†…å­˜ä¸è¶³ï¼Œå°è¯•å¯ç”¨æ¨¡å‹å¸è½½...")
                    notes = "å†…å­˜ä¸è¶³ï¼Œéœ€è¦å¸è½½æˆ–æ›´å¤šGPU"
                else:
                    notes = f"è¯„ä¼°å¤±è´¥: {ve}"
                    print(f"è¯·æ±‚ {request_id} å¤±è´¥: {ve}")
            except Exception as e:
                notes = f"è¯„ä¼°å¤±è´¥: {e}"
                print(f"è¯·æ±‚ {request_id} æ„å¤–é”™è¯¯: {e}")
            
            # ä¿å­˜ç»“æœï¼ŒåŒ…å«å¤šèŠ‚ç‚¹ä¿¡æ¯
            results.append({
                'Request_ID': request_id,
                'Model': 'deepseek-r1',
                'Batch': batch_size,
                'TP': tensor_parallel_nodes,
                'PP': pipeline_parallel_nodes,
                'Num_Nodes': num_nodes,
                'Node_Topology': topology,
                'Interchip_BW(GB/s)': interchip_link_bw,
                'Interchip_Latency(us)': interchip_link_latency,
                'Input_Tokens': input_tokens,
                'Output_Tokens': output_tokens,
                'Beam_Size': beam_size_value,
                'TTFT(ms)': ttft_ms,
                'TPOT(ms)': tpot_ms,
                'E2E(ms)': e2e_ms,
                'Prefill_Throughput(tokens/s)': prefill_throughput,
                'Decode_Throughput(tokens/s)': decode_throughput,
                'Notes': notes
            })
        
        # ä¿å­˜åˆ°CSV
        results_df = pd.DataFrame(results)
        results_df.to_csv(output_csv_filename, index=False)
        print(f"\nâœ… MoEæ¨¡æ‹Ÿç»“æœå·²ä¿å­˜åˆ°: {output_csv_filename}")
        
        # ç»Ÿè®¡åˆ†æ
        successful_sims = results_df['TTFT(ms)'].notna().sum()
        print(f"æˆåŠŸæ¨¡æ‹Ÿ: {successful_sims}/{len(results_df)} ä¸ªè¯·æ±‚")
        
        if successful_sims > 0:
            print("\n=== MoEæ¨¡å‹æ€§èƒ½ç»Ÿè®¡ ===")
            print(f"å¹³å‡ TTFT: {results_df['TTFT(ms)'].mean():.2f} ms")
            print(f"å¹³å‡ TPOT: {results_df['TPOT(ms)'].mean():.2f} ms") 
            print(f"å¹³å‡ E2E:  {results_df['E2E(ms)'].mean():.2f} ms")
            
            # MoEç‰¹æœ‰åˆ†æ
            if not results_df['Prefill_Throughput(tokens/s)'].isna().all():
                print(f"é¢„å¡«å……ååé‡: {results_df['Prefill_Throughput(tokens/s)'].mean():.2f} tokens/s")
            if not results_df['Decode_Throughput(tokens/s)'].isna().all():
                print(f"è§£ç ååé‡: {results_df['Decode_Throughput(tokens/s)'].mean():.2f} tokens/s")
        
        return results_df
    
    def create_test_workload(self, workload_type='mixed'):
        """åˆ›å»ºæµ‹è¯•å·¥ä½œè´Ÿè½½"""
        
        if workload_type == 'chat':
            # å¯¹è¯åœºæ™¯
            data = [
                {'input_tokens': 1024, 'output_tokens': 256},
                {'input_tokens': 2048, 'output_tokens': 512},
                {'input_tokens': 4096, 'output_tokens': 1024},
            ]
        elif workload_type == 'code':
            # ä»£ç ç”Ÿæˆåœºæ™¯
            data = [
                {'input_tokens': 2048, 'output_tokens': 512},
                {'input_tokens': 4096, 'output_tokens': 1024},
                {'input_tokens': 8192, 'output_tokens': 2048},
            ]
        elif workload_type == 'long_context':
            # é•¿æ–‡æœ¬åœºæ™¯
            data = [
                {'input_tokens': 16384, 'output_tokens': 1024},
                {'input_tokens': 32768, 'output_tokens': 2048},
                {'input_tokens': 65536, 'output_tokens': 4096},
            ]
        else:  # mixed
            # æ··åˆåœºæ™¯
            data = [
                {'input_tokens': 1024, 'output_tokens': 256},
                {'input_tokens': 2048, 'output_tokens': 512},
                {'input_tokens': 4096, 'output_tokens': 1024},
                {'input_tokens': 8192, 'output_tokens': 1024},
                {'input_tokens': 16384, 'output_tokens': 2048},
            ]
        
        df = pd.DataFrame(data)
        return df


def main():
    """ä¸»å‡½æ•°"""
    
    parser = argparse.ArgumentParser(description="DeepSeek-R1 MoE æ¨¡å‹æ€§èƒ½è¯„ä¼°")
    
    # ç¡¬ä»¶é…ç½®å‚æ•° (RTX 4090é…ç½®ï¼Œé’ˆå¯¹32å¡é›†ç¾¤ä¼˜åŒ–)
    parser.add_argument("--flops", type=float, default=330, help="TFLOPs (RTX 4090: ~330)")
    parser.add_argument("--mem_size", type=float, default=24, help="Memory size (GB, RTX 4090: 24GB)")
    parser.add_argument("--mem_bw", type=float, default=1008, help="Memory BW (GB/s, RTX 4090: ~1008)")
    parser.add_argument("--icn_bw", type=float, default=16, help="ICN BW (GB/s, NVLink/PCIe)")
    
    # è¯„ä¼°é…ç½®
    parser.add_argument("--quantization", type=str, default="bf16", 
                       choices=["fp32", "f32", "fp16", "bf16", "int8", "int4", "fp8"],
                       help="é‡åŒ–è®¾ç½®")
    parser.add_argument("--beam_size", type=int, default=1, help="æŸæœç´¢å¤§å°")
    parser.add_argument("--system_eff", type=float, default=0.8, help="ç³»ç»Ÿæ•ˆç‡")
    
    # å¹¶è¡Œé…ç½®
    parser.add_argument("--tp_nodes", type=int, default=8, 
                       help="å¼ é‡å¹¶è¡ŒèŠ‚ç‚¹æ•° (MoEæ¨¡å‹æ¨è8+)")
    parser.add_argument("--pp_nodes", type=int, default=4, help="æµæ°´çº¿å¹¶è¡ŒèŠ‚ç‚¹æ•°")
    parser.add_argument("--enable_offload", action="store_true", help="å¯ç”¨æ¨¡å‹å¸è½½")
    
    # å¤šèŠ‚ç‚¹é…ç½®å‚æ•°
    parser.add_argument("--num_nodes", type=int, default=4, help="èŠ‚ç‚¹æ•°é‡")
    parser.add_argument("--interchip_bw", type=float, default=10, 
                       help="èŠ‚ç‚¹é—´äº’è¿å¸¦å®½ (GB/s)")
    parser.add_argument("--interchip_latency", type=float, default=1.9, 
                       help="èŠ‚ç‚¹é—´é€šä¿¡å»¶è¿Ÿ (Î¼s)")
    parser.add_argument("--topology", type=str, default="FullyConnected",
                       choices=["FullyConnected", "Ring", "2DTorus"], 
                       help="ç½‘ç»œæ‹“æ‰‘ç»“æ„")
    
    # å·¥ä½œè´Ÿè½½
    parser.add_argument("--workload", type=str, default="mixed",
                       choices=["chat", "code", "long_context", "mixed"],
                       help="æµ‹è¯•å·¥ä½œè´Ÿè½½ç±»å‹")
    parser.add_argument("--input_csv", type=str, default="/home/wang/sim/bench/deepseek/chat.csv",
                       help="è¾“å…¥CSVæ–‡ä»¶è·¯å¾„(å¯é€‰ï¼ŒåŒ…å«input_tokenså’Œoutput_tokensåˆ—)")
    
    # è¾“å‡º
    parser.add_argument("--output_csv", type=str, default="deepseek_r1_moe_results.csv",
                       help="è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    print("DeepSeek-R1 MoE æ¨¡å‹æ€§èƒ½è¯„ä¼°å·¥å…·")
    print("="*50)
    
    try:
        # åˆå§‹åŒ–è¯„ä¼°å™¨
        config_path = os.path.join(os.path.dirname(__file__), 'ds-r1-config.json')
        evaluator = DeepSeekR1MoEEvaluator(config_path)
        
        # åˆ›å»ºç¡¬ä»¶é…ç½®
        hardware_config = {
            'Flops': args.flops,
            'Memory_size': args.mem_size,
            'Memory_BW': args.mem_bw,
            'ICN': args.icn_bw,
            'real_values': True
        }
        
        print(f"\n=== ç¡¬ä»¶é…ç½® (å•èŠ‚ç‚¹) ===")
        print(f"ç®—åŠ›: {args.flops} TFLOPs")
        print(f"æ˜¾å­˜: {args.mem_size} GB") 
        print(f"å†…å­˜å¸¦å®½: {args.mem_bw} GB/s")
        print(f"äº’è¿å¸¦å®½: {args.icn_bw} GB/s")
        
        print(f"\n=== é›†ç¾¤æ€»é…ç½® ({args.num_nodes}èŠ‚ç‚¹) ===")
        print(f"æ€»ç®—åŠ›: {args.flops * args.num_nodes} TFLOPs")
        print(f"æ€»å†…å­˜: {args.mem_size * args.num_nodes} GB")
        
        print(f"\n=== è¯„ä¼°é…ç½® ===")
        print(f"é‡åŒ–: {args.quantization}")
        print(f"å¹¶è¡Œé…ç½®: TP={args.tp_nodes}, PP={args.pp_nodes}")
        print(f"ç³»ç»Ÿæ•ˆç‡: {args.system_eff}")
        
        print(f"\n=== å¤šèŠ‚ç‚¹é…ç½® ===")
        print(f"èŠ‚ç‚¹æ•°é‡: {args.num_nodes}")
        print(f"èŠ‚ç‚¹æ‹“æ‰‘: {args.topology}")
        print(f"äº’è¿å¸¦å®½: {args.interchip_bw} GB/s")
        print(f"äº’è¿å»¶è¿Ÿ: {args.interchip_latency} Î¼s")
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        if args.input_csv and os.path.exists(args.input_csv):
            print(f"\nä½¿ç”¨è‡ªå®šä¹‰CSV: {args.input_csv}")
            df = pd.read_csv(args.input_csv)
        else:
            print(f"\nä½¿ç”¨é¢„è®¾å·¥ä½œè´Ÿè½½: {args.workload}")
            df = evaluator.create_test_workload(args.workload)
        
        print(f"æµ‹è¯•è¯·æ±‚æ•°: {len(df)}")
        
        # è¿è¡Œè¯„ä¼°
        print(f"\nå¼€å§‹ DeepSeek-R1 MoE æ€§èƒ½è¯„ä¼°...")
        results_df = evaluator.run_batch_simulation_from_df(
            df=df,
            hardware_config=hardware_config,
            quantization_setting=args.quantization,
            beam_size_value=args.beam_size,
            system_efficiency_value=args.system_eff,
            tensor_parallel_nodes=args.tp_nodes,
            pipeline_parallel_nodes=args.pp_nodes,
            model_offload_enabled=args.enable_offload,
            output_csv_filename=args.output_csv,
            # å¤šèŠ‚ç‚¹å‚æ•°
            num_nodes=args.num_nodes,
            interchip_link_bw=args.interchip_bw,
            interchip_link_latency=args.interchip_latency,
            topology=args.topology
        )
        
        print(f"\nâœ… è¯„ä¼°å®Œæˆï¼Œç»“æœä¿å­˜è‡³: {args.output_csv}")
        
        print("\n=== MoEæ¨¡å‹ç‰¹æ€§æç¤º ===")
        print("1. DeepSeek-R1 ä½¿ç”¨ MoE æ¶æ„ï¼Œå®é™…è®¡ç®—é‡æ¯”å‚æ•°é‡å°")
        print("2. å»ºè®®ä½¿ç”¨ FP8 é‡åŒ–ä»¥èŠ‚çœå†…å­˜")
        print("3. å¼ é‡å¹¶è¡Œåº¦å»ºè®® 8+ ä»¥å……åˆ†åˆ©ç”¨ä¸“å®¶å¹¶è¡Œæ€§")
        print("4. é•¿åºåˆ—åœºæ™¯ä¸‹æ³¨æ„ KV ç¼“å­˜å†…å­˜å ç”¨")
        
    except FileNotFoundError as e:
        print(f"âŒ é…ç½®æ–‡ä»¶é”™è¯¯: {e}")
        print("è¯·ç¡®ä¿ ds-r1-config.json æ–‡ä»¶å­˜åœ¨")
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥é…ç½®å‚æ•°å’Œç¯å¢ƒ")


if __name__ == "__main__":
    main()
